---
title: 《kafka权威指南》- 读书笔记
categories: 
    - [kafka]

tags: 
    - 读书笔记

excerpt: 相信在实际工作中，MQ使用的使用大部分人都接触过. RocketMQ，Kafka. 这里对kafka的一些基本知识做一些总结

---



<a name="ZazXK"></a>
## chapter 2-安装


<a name="nbCIi"></a>
### 1. zookeeper

- zk的作用: 存储消费者信息+topic信息+kafka集群的信息
- zk集群会有选举，所以节点数量最好是奇数以免形成脑裂. 但一个zk集群不建议包含超过7个节点，因为节点过多时在进行集群一致性统一时，会消耗过多资源造成整体集群的性能下降
- zk的几个端口
   - clientPort: 客户端连接到zk集群所用的端口，默认是2181
   - peerPort: 节点与节点之间平时进行数据通信时所用的TCP端口
   - leaderPort: 用于集群选举时通信用的TCP端口


<br />

<a name="zaS5H"></a>
### 2. kafka broker配置

- port: kafka服务端开放给客户端连接的端口, 默认是9092
- brokder.id:当前broker在kafka集群中的唯一编号
- zookeeper.connect: zk的地址，这里的zk就是上面说的用于存储kafka集群信息+topic信息+消费者信息的
- log.dirs: 存放数据的目录。 kafka的消息都存放在这些目录中，该参数是一组以逗号隔开的文件系统路径。当有多个文件系统路径时，Kafka存放消息的时候会往最空闲(存的数据最少)的目录上放
- num.recovery.threads.per.data.dir: broker服务器启动/关闭时，每个log.dir 会有多少个线程去处理里面的数据
- auto.create.topics.enabe: 当topic不存在的时候是否会默认生成？ 该配置默认是true



<a name="VUATP"></a>
### 3. kafka topic的配置

- num.partitions
- 消息存留策略
   - log.retention.ms
   - log.retention.bytes
- 日志片段关闭的策略: 当一个日志片段关闭之后，新的日志片段才会生成
   - log.segment.bytes. 关闭某个segment的最小bytes，当一个topic一天只能收到1M的数据，而该参数设置的值是10M，那么需要10天才能达到阀值。这10天内该segment是不会过期的. 如果log.rention.ms的值是7天，那么一个segment的生命周期就是10+7=17天
   - log.segment.ms: 关闭某个segment的最小ms数
- message.max.bytes: 单个消息的最大byte数. 默认是1M
   - 该值指的是压缩后消息大小
   - 如果生产者发送超过该大小的消息，broker不会处理该消息，同时会返回错误信息


<br />

<a name="W28mb"></a>
### 4. 硬件选择的影响

- 对生产者的影响
   - 磁盘的吞吐量. 吞吐量越大，数据写入速度就越快，生产者生产消息的时候的延时就越低
   - 磁盘容量。越大就能存越多的数据
- 对消费者的影响
   - 内存: 当读取消息的时候，会将消息读取到broker的内存中，所以内存会影响消费者的读取速度. 一般建议不要跟其他应用公用一个内存，即最好kafa服务器中没有其他的应用程序在一起跑
- 对生产者/消费者都有影响
   - 网络: 如果集群的网络带宽被沾满，那么写入数据/读取数据都会收到影响，同时集群内的数据复制也会占用带宽，这样就会造成性能很差的表象
   - CPU: 消息的传递需要压缩/解压缩，所以会消耗较多的CPU。 但是CPU对kafka的影响还是比较小的


<br />

<a name="JEcX4"></a>
### 5. 其他

- 集群中broker的配置
   - 大家指向的zk地址是同一个，这样才能通过zk来统一管理
   - broker.id在集群内是唯一的。如果有两个broker.id是一样的，那么会有一个无法启动
- 数据中心布局
   - kafka集群虽然会有备份的说法，但是如果备份的数据跟主数据在同一个机房上，一旦机房出现故障，数据就会丢失，起不到备份的作用。所以最好是不同的机房+不同的机架进行数据备份
- 对于zookeeper
   - 存储kafak集群信息+消费者元数据+topic数据
   - 在kafka 0.9.0.0之后，消费者元数据信息已经放到了broker中. 这样就减少了zk的压力
   - 最好zk集群不要跟其他应用程序一起使用，以免相互污染


<br />

<a name="6UHdR"></a>
## kafka producer
kafka producer包含以下几个信息

- topic
- partition
- key
- value


<br />在发送一个消息的时候，producer会对消息进行

- 通过key进行分区计算，得出当前消息应该去哪个区。这就是分区器
- 对key/value进行序列化，这就是序列化器。当然在consumer拿到数据的时候会进行反序列化
- 以上两个都可以自定义


<br />
<br />在创建一个producer的时候，我们需要指定

- bootstrap.servers: broker集群中的某几个broker的地址清单，以 host:port 形式给出。需要注意的是不一定需要将所有的broker地址都列出来，只要给出一两个即可。producer可以通过其中一个找到其他的broker。但是清单中的最好大于等于2个，以免一个broker挂了可以通过另一个去找到broker集群
- key.serializer
- value.serializer


<br />
<br />producer的一些配置

- acks: 有多少个broker回应了成功，这一次写数据才算成功？ 0表示不用broker恢复，扔完消息就不管了; 1表示只要有一个就可以，这里的1个一般都是leader broker; 当需要所有broker都回复了，可将该值设置为all，这也是最安全的方式，但是也是最耗时的
- compression.type: 对消息的压缩。默认是没有的
- retries: 失败重试的次数。有时候失败是由于broker集群刚好在换届选举，此时重试下成功的概率将很大。kafka默认是会重试的
- 缓冲相关的配置
   - buffer.memory: 当往服务器发送消息，被堵住时，producer所在机器最多可以缓冲多少接下来想要发送的消息而不报错.
   - batch.size： 一个批次的数据大小, 到了这个值就会出发一次网络IO。但需要注意的是并不是每次都要达到该值才会触发网络传输，有时候达到一半就会触发，这跟linger.ms有关。kafka producer并不会每来一个消息就发送，而是会等到有一定量的时候才一次将消息发送到brokder，这样可以减少网络传输，最大化效率
   - linger.ms: 发送批次之前，等待多长时间
   - max.block.ms: 当producer的发送缓冲区已经满了，即buffer.memory已经达到了设置的值，或者获取不到元数据信息了(存储的时候存储不下了)，此时会等待max.block.ms时间，当时间到了就抛异常
- 通信相关
   - max.request.size: 一次请求所包含的最大的消息的大小. 该消息可以是一个消息，也可以是由多个消息组成。比如该值为1M，那么可以一次请求可以传输一个1M的消息，也可以传输10个1KB的消息
   - max.in.flight.request.per.connection: producer发送消息之后在收到服务器回应之前可以发送多少个消息。值越大占用producer所在机器的内存越大，当然吞吐量也越大。当该值为1时，能保证消息的顺序


<br />
<br />关于分区

- 如果消息的key为null，那么将会使用默认的分区器，此时会采用round robin(轮询)的方式，随机将消息投递到某个分区
- 如果消息的key不为null，并且没有指定分区器，则也会使用默认的分区器，此时
   - 对key进行hash云端，得到一个值
   - 然后将这个值映射到broker集群下的某个分区。这里需要注意的是
      - 只要集群没有新增分区，那么该key会被一直分配到同一个broker；但当分区新增了，那就会投到别的分区了
      - 会将集群下的所有分区都拿过来进行映射，无论该分区是否可用。如果选中的那个分区不可用，那么会报错。这也解释了为什么是增加分区，而不是说加少分区会对映射结果有影响


<br />
<br />

<a name="sa4Cv"></a>
## chapter 4 - consumer

<br />一个topic的消费者群组中的consumer个数需要比该topic内的分区数少，否则会出现某些consumer没有partition可处理，造成资源闲置<br />
<br />消费者获取数据，是通过主动拉取的方式获取的，调用的是 consumer.poll方法. 在调用该方法时，会做好多事情，包括

- 将自己加入到 consumer group中
- 给群组协调器的broker发送心跳，告诉它我还活着
- 提交上一次的处理完的消息的偏移量 (在自动提交的时候)


<br />

<a name="URXV7"></a>
### 1. 再分配

- 当群组中有新的consumer加进来或者有consumer死去，那么consumer group就会对分区做再分配的动作, 即topic的分区所有权可能会从consumer1转移到其他consumer上
- 实现过程:
   - 第一个加入群组的consumer会成为群主
   - 群主会从 协调器broker中获取当前群主的所有 consumer信息
   - 然后群主根据一定的策略，给每个consumer分配它需要处理的partition。策略目前有两个
      - range: 该策略可能会出现某个consumer明显多，某个consumer明显少的情况
      - roundRobin: 轮询分配，该分配策略分配的比较平均
   - 最后群主会将分配情况提交给 协调器broker; 然后协调器broker会给每个consumer发送它自己分配到的partition。这样，只有群主知道其他consumer的partition分配情况；而其他的consumer只知道自己分配到的partition情况，看不到其他人的
- 再分配期间，整个群组将不可用，即不能消费消息


<br />
<br />在实践中，推荐一个consumer一个线程，不要一个线程里有多个consumer；这样做到互不相干，不会互相影响。<br />
<br />

<a name="SrmEB"></a>
### 2. 消费者的一些参数配置

- 一次能获取到的数据
   - max.partition.fetch.bytes ： 代表consumer一次能从每个partition中获取到的消息的最大字节数。该值不能小于topic配置中的 max.message.size. 否则会造成消费者获取不到数据，从而一直在重试
   - fetch.min.bytes: 一次拿到的数据的最小字节数。少于该值将会等待，这有助于减少网络IO次数
   - fetch.max.wait.ms： 该参数表示在拿不到fetch.min.bytes数量的字节数的情况下，等待多久就返回。
- 偏移量相关
   - auto.offset.reset: 消费者在读取一个没有偏移量的分区或者该偏移量无效的时候应该怎么办？
      - 默认值是latest，表示从最新的记录开始读取；
      - 还有一个值是earliest，表示从起始位置开始读取数据
   - enable.auto.commit: 是否自动提交偏移量。默认是true. 并且默认的间隔时间是5s.


<br />

<a name="xSkTX"></a>
### 3. 偏移量&提交

- 实现机制: 偏移量的提交，是消费者往一个叫做 "_consumer_offset"的特殊topic发送消息，记录自己消费的偏移量信息。
- 提交方式
   - 自动提交: 使用kafka默认的机制提交。但这个策略容易造成消息的重复消费，因为两次提交之间的间隔内可能发生再均衡了
   - 手动提交，能更精确的控制
      - 同步提交: 比较安全，但是会堵塞住处理线程，降低吞吐量
      - 异步提交: 能快速提交，并且借助回调函数能知道成功与否. 但会出现时间上是前面提交的偏移量，但是却是更后面达到，从而会有覆盖的现象. 比如11:30提交了2000的偏移量，但是发生了短暂的通信问题，导致服务器收不到；与此同时我们处理了另外一批数据，并成功提交了偏移量3000。如果我们进行重试，那么偏移量为2000的可能会覆盖偏移量3000
      - 同步+异步提交: 处理的时候通过异步方式实现，这样快速；一旦出现异常时(比如网络不稳定)，则使用同步做补偿，由于同步会一直重试直至成功或者发生无法恢复的错误才会停止，所以同步能确保最后处理的偏移量能提交成功



- 其他
   - 我们可以监听 再均衡事件，对其进行处理. 方式是: 在consumer.subscribe(topic, listener)
   - 我们也可以通过 seek的方式，从我们想要的偏移量位置上开始处理数据。这在数据丢失或者重复的时候，比较有用
   - 在程序关闭的时候，可以通过在ShutdownHook调用 consumer.wakeup方法让 consumer.poll 抛出异常，然后在finally中调用consumer.close对 消费者占用的资源做清理


<br />

<a name="8RBdk"></a>
## chapter 5 - 深入Kafka


<a name="SZLjE"></a>
### broker集群
<a name="R6OV9"></a>
#### 1.集群成员关系
一个独立运行的kafka服务器，被称为broker。多个broker配置的zk信息是同一个，那么它们就会形成一个kafka集群

- 每个broker注册到集群的时候，都是以一个zk临时节点的形式出现的，它们会被挂载到目录` brokers/ids`下面
- 每个broker都会有一个id. 如果它的id已经有其他bro ke r在用了，那么它会启动失败
- 当一个broker失联了，那么它对应的zk临时节点会被移除



<a name="CEU5r"></a>
#### 2.控制器broker
一个topic会有多个分区，每个分区会落到一个broker上. 同时每个分区又会有多个副本，以做灾备之用. 所以这时候对于每个分区而言，就需要选举一个leader分区来，这样该分区的副本分区就会从leader分区复制数据。这个leader分区的选举工作是由一个叫做 控制器(controller)的broker来负责的。<br />
<br />控制器broker具有以下特点

- 控制器broker本质上也是一个broker，所以它具有普通broker的功能. 
- 控制器broker还负责分区的选举工作，这是它特有的功能


<br />控制器broker选举过程: 

- 集群中的每个broker在启动的时候，都会去zk上注册一个名叫 `/controller`的节点，以让自己成为控制器broker
- 当发现该节点已经存在的时候，就知道控制器broker已经名花有主了. 当前的broker就会去watch 该节点，这样就可以收到该节点的变更信息. 
- 每个controller都有自己的任期(也叫纪元)，该值通过epoch来表示
- 当控制器broker挂了的时候，其他节点会收到通知，然后就会尝试去争当controller. 当一个新的节点成为了controller之后，它的epoch会比之前的大；
- 通过epoch，即使前任controller重新连接到集群里来，也无法成为controller了，因为那些普通broker知道自己的controller当前的epoch是多少，当收到小于该epoch的消息，它们会忽略


<br />

<a name="HxLmq"></a>
#### 3. 复制
复制就是副本分区从leader分区copy数据的过程. 这个过程是顺序执行的，即副本分区在请求消息5的时候，它肯定已经完成了消息1/消息2/消息3/消息4的复制<br />
<br />跟leader分区的数据保持一致的分区，被称作同步副本。在首领失踪时，这些副本是有可能被选为新首领的，因为它们拥有的消息是最新的。<br />在选举谁是leader分区时，有一个首选首领的概念。 首选首领，是在分区创建的时候被选中的首领，也就是初代首领。当在某轮选举之后，首领不是首选首领，并且此时首选首领是同步副本，那么会触发选举，让首选首领成为首领。这么做的原因是，首领的负载是比较高的，一般初代首领能承受该压力(一般来说它的配置会好一些)，这样集群内的负载能达到均衡，不至于使负载能力不够高的broker负重过高，造成整体集群的不稳定。<br />
<br />
<br />

<a name="JzHEs"></a>
### 请求处理
kafka的所有消息，都是标准的，都会包含:

- request type - 请求类型
- request version - 请求的版本，这在客户端版本不一致时有用. 
- correlation id - 请求消息的唯一标识，在问题诊断时用处较大
- client id - 用于表示请求的客户端


<br />
<br />

<a name="si0iv"></a>
#### 1. 请求的处理过程

1. broker会在它所监听的每个端口上创建一个Acceptor线程。该线程会创建一个链接，并将该链接交给Processor线程去处理。Processor线程也叫做网络线程
1. 当请求过来的时候，processor线程会将请求放到一个请求队列中
1. 然后I/O线程就会从请求队列中获取数据，进行处理；处理完之后将结果放到响应队列中，然后再通过processor线程传送回客户端


<br />

<a name="zzdrp"></a>
#### 2. 请求种类
<a name="dELMg"></a>
##### 2.1.元数据请求
所谓元数据就是包含topic有哪些分区，分区的副本信息，分区的首领是谁等信息。客户端通过该信息可以知道将请求发送到哪个broker(这里就是首领分区所在的broker)。客户端会缓存元数据，那么它如何保证自己的元数据是罪行的呢？

- topic内的所有broker的元数据信息都是一样的
- 当请求的时候，目标broker不是首领broker，那么目标broker会将当前最新的元数据信息返回给客户端。 客户端收到之后就知道自己的元数据信息已经过时了，会做更新并将请求发送到leader broker上去
- 同时客户端会定时的主动请求元数据信息，以使自己不落伍


<br />

<a name="zbS6X"></a>
##### 2.2.生产请求
生产请求，主要有一个acks参数. 该参数表示有多少的副本拥有了该消息才算成功. 该参数有三个值

- 0, 表示不用复制，发出去就收工. 不用管是否到达
- 1。表示只要leader broker收到就可以了
- all，表示分区内的所有副本都需要成功收到该消息才做发送成功


<br />需要注意的是: broker在收到消息的时候，是将消息写入操作系统的缓存中的，此时并没有写入到磁盘中. 何时写入磁盘取决于操纵系统<br />
<br />

<a name="isQNl"></a>
##### 2.3.读取请求
读取的时候有两个重要参数:

- 限制broker最多可以从一个分区返回多少的数据；这个参数可以防止分区返回过多的数据，造成消费端的内存不够的现象
- 等待多久之后如果分区还是没有那么多数据，就返回；这个参数防止broker生产的数据太少，等待太久


<br />同时需要注意的是，当生产者在生产数据的时候，该数据的副本数没有达到acks的数量时，consumer是读取不到该消息的。因为假如可以读取到，当leader broker突然挂了，此时acks的数量还不够，那么新的leader broker很可能就没有那条消息，这就会造成再次读取的时候发现读取不到那条消息了，从而导致数据的不一致<br />
<br />

<a name="CFmss"></a>
##### 2.4.其他请求
由于kafka的消息是使用标准的协议进行流转的，所以kafka可以实现跨语言的交流。即java客户端可以读取到go客户端生产的数据。<br />关于客户端跟服务器(即broker)之间的版本升级的问题，需要注意的是: 一定是先升级服务端，然后再升级客户端。

- 因为客户端发送请求的时候，会指定自己当前是什么版本。如果该版本在服务端是不支持的，那么服务端将不知道如何处理。反过来则可以处理


<br />

<a name="kUkAq"></a>
### 物理存储


<a name="OykGr"></a>
#### 1. 分区分配规则
分区是kafka最基本的存储单元了，所以在进行分区分配的时候，应尽量做到容灾

- 分布均匀的分配在不同的broker中
- 同一个分区的不同副本尽量不要放在同一个broker. 防止一个broker挂了，数据丢失
- 如果有条件搞多机房多机架，那么也要将分区的副本分不到不同的机房机架上去


<br />

<a name="ddzUg"></a>
#### 2. segment与句柄
由于在一个大文件里查找和删除都是很费时且容易出错的，所以kafka把分区分成多个片段(segment). 默认情况下，每个片段包含1GB或者一周的数据，以较小者为准。在写入数据的时候，如果一个片段达到它的上限，那么他会被关闭，然后开启一个新的segment来处理数据。<br />这里需要注意的是，操作系统会为每个片段都打开一个句柄，即使该片段不活跃。这样就会造成操作系统的句柄会被快速地用光，所以可以根据实际情况对操作系统做一定的调优。<br />
<br />

<a name="J3C60"></a>
#### 3. 零复制技术
kafka的消息在网络传输，磁盘上保存，都是同样的数据格式。这个特性可以使数据在读取的时候实现零复制，即直接从文件系统中将消息读取出来，然后直接返回给客户端，而不需要在broker的内存中进行复制解压打包等工作，从而不需要在broker上分配额外的内存去处理。 consumer端拿到数据之后，按照格式进行解压处理即可<br />
<br />这里有个需要注意的地方，就是如果任意一端对数据格式的改动，其他各端都需要跟着改，否则数据就不能兼容处理了。 比如生产端加了某些东西，那么consumer侧也需要做处理，否则将处理不了新数据<br />
<br />
<br />

<a name="hLZie"></a>
#### 4. 索引
kafak为每个分区都维护了一个索引，以快速读取数据。该索引维护的是消息的偏移量跟片段文件的关系，以及偏移量在片段文件内的位置<br />
<br />

<a name="jz2IL"></a>
#### 4. 清理
当数据重复或者存在脏数据时，需要对数据进行清理。可通过参数 log.cleaner.enabled 来开启清理工作。每个日志片段的数据，都可以分成两部分:

- 干净的部分，这部分数据是上一次清理之后留下来的
- 污浊的部分，这部分数据是上一次清理之后产生的



清理的大致步骤如下

1. 在内存中生成一个map，将污浊部分的数据写入该map，key为消息的键的hash值，value为消息的偏移量
1. 由于是从污浊部分数据的开头开始遍历写的，所以如果某个key在污浊数据内存在多次，那么最后一次的数据将保留
1. 对日志片段的数据进行遍历，从干净部分的数据开始
1. 如果数据在map中不存在，说明该数据是新数据，移到替换的片段上
1. 如果数据在map中存在，说明该数据在污浊部分是存在的，则丢弃当前的
1. 然后将map中的数据，移到替换片段上
1. 最后将替换片段与原始片段交换，从而实现数据的清理


<br />
<br />墓碑消息: key存在，但是其value为null的消息

<a name="V4MUp"></a>
## chapter 6 - 可靠的数据保证

<br />kafka的数据可靠性是通过复制和分区的多副本来保证的。kafka的topic保存在多个分区中，每个分区可以有多个副本，每个副本对应一个broker。所以每个分区都有一个leader，用来对外接受/处理事件，其他的broker则是follower。follower会定时的同步leader的信息，以此来保证自己是同步副本<br />
<br />同步副本需要满足以下条件

- 过去6s内向zookeeper发送过心跳
- 过去10s从leader那里获取过消息
- 过去10s从leader那里获取过最新的消息. 光从首领那里获取消息是不够，还必须是最新的消息


<br />可以通过kafka的几个重要组建的配置来做一些保证<br />

<a name="38c860c1"></a>
#### 1. broker的配置
| 参数 | 配置项 |  |
| --- | --- | --- |
|  |  |  |
|  |  |  |

<a name="ul1mo"></a>
##### 1.1. 复制参数
该参数表示需要多少个副本，可以在topic层级进行配置，也可以在broker层面配置

- topic级别的参数: replication.factor 
- broker层级的参数: default.replication.factor - 表示自动创建的主题的将会有多少个副本

该值越大可靠性越高，单会消耗过多的资源; 该值越小则可靠性不可保障<br />

<a name="l4IB5"></a>
##### 1.2. 不完全首领选举
该参数是broker级别(实际上是集群范围内，因为只有broker集群才有选举)的，用于当首领处于不可用的时候，是否可以将非同步副本选举为首领。参数名为: unclean.leader.election, 默认为true

- 如果允许则需要忍受消息丢失的问题
- 如果不允许，则需要忍受不可用的情况(首领不可用，其他副本又都不是同步副本，那就只能等首领重新活过来了)



<a name="xi2cE"></a>
##### 1.3. 最少同步副本
该参数表示kafka写消息的时候，需要多少个副本的确认才认为该消息写入成功。当broker集群中的同步副本少于该配置值时，写消息将失败，此时集群只能提供读功能。这是一个可靠性跟可用性之间的一个权衡<br />在broker跟topic上都可以配置该参数，并且它们都叫: min.insync.replicas<br />
<br />

<a name="ypGjC"></a>
#### 2. producer
生产者在写入消息的时候，很重要的一个参数是acks，表示需要有多少个副本的确认才认为是成功的。同时在写入消息时可能发生错误，此时可以通过重试来提供成功率。但这里需要注意的是并不是所有的错误都是可以重试的

- 可重试的错误，一般是网络问题或者 broker集群在选举返回LEADER_NOT_AVAILABLE
- 不可重试的错误，一般是配置错误导致的，比如INVALID_CONFIG这种错误


<br />重试带来的风险是，数据可能会重复写入。此时最好在消息中带上一个唯一ID，以方便自己或者consumer做幂等处理<br />
<br />

<a name="ftZ0n"></a>
#### 3. consumer
消费者读取消息都是根据偏移量来进行的，所以如果提交了偏移量但是没有处理完消息就挂了，那么“其他”消费者过来顶替的时候，之前那条消息则永远不会被消费，因为偏移量已经提交了。所以一般都是在处理完了消息之后再进行偏移量的提交的，也正因为该原因，一般都是显式提交偏移量。显式提交偏移量，可能需要注意

- 在消费消息的时候，如果失败了，可以将该消息写入其他地方或者写入一个专门的topic，稍后再处理。这样可以保证后面的消息可以正常得到处理，而不影响总体进度
- 如果消息过大，或者处理的时候需要做一些IO等操作，可以通过使用线程池来来提高吞吐量
- 有时候我们需要消费不只是"at least once" 而是"exactly once"，此时就可以借助其他系统来保证了，比如借助DB的唯一主键来达到幂等消费


<br />

<a name="6iRJ2"></a>
## Chapter 11 - 流式处理
事件流，也叫数据流，是一种无边界的事件集合的抽象表示. 在该流中，事件会源源不断的产生，然后被处理，Java 8中的stream就是这么一个东西。事件流除了无边界外，还有其他一些特性

- 有序性。事件的发生总是有时间的先后顺序的
- 数据不可变性。只要生成了，就不会改变，就像MySQL的binlog记录数据库的所有操作一样
- 可重播性. 对历史事件流可以重播，比如重播发生在几个月前的数据，而这也是事件流的一个较大的特性


<br />
<br />流式处理是指实地处理一个或多个事件流，跟Request-Response，批处理一样，流式处理也是一种编程范式

- request-response: 大部分的请求都是该类型，比如在页面上查询某个商品的价格，请求之后就等在那里，知道有响应返回或者超时. 该范式的特点是: 数据量小，响应时间快
- 批处理，是指对数据进行批量处理，比如对一堆数据进行处理然后生成报表供老板看。这种一般都是对冷数据的处理，数据量比较大耗时较严重，不会要求有很高的实时性。
- 流式处理，则是介于上面两者之间. 它既不需要响应很及时，但是又不是说过个几个小时才有响应. 同时它一次处理的数据不会非常大，但也不小


<br />
<br />

<a name="lBNX2"></a>
#### 流式处理的一些概念

- 时间. 流式处理中有三个时间
   - 事件时间，该时间是事件发生的时间
   - 日志追加时间. 改时间是事件被存储到某个地方的事件，比如kafka中则是存入到broker中
   - 处理时间，该时间则是事件被处理的事件. 这个时间不是很准确，比如事件被两个线程处理了，那么它就会有两个处理时间
- 状态. 事件跟事件之间的信息被称为状态，一般是事件与事件之间有关联性时会用上。状态的类型有
   - 本地状态/内部状态. 该状态存储在应用实例的本地，不能被别的实例所看到
   - 外部状态，该状态是可以在各个实例间共享，一般是通过外部数据存储来维护. 尽量避免这类状态，因为这个很影响流处理过程的时延
- 流跟表的关系.
   - 流包含数据的每个变更
   - 表，只包含数据的最新状态.
- 时间窗口，大部分流式处理都是基于事件窗口的，时间窗口在选择上需要注意几点
   - 窗口的大小. 窗口越小，就能越快地发现变更，但是噪音也越多. 窗口越大，变更发现的越慢，但是对应的变更越平滑准确
   - 窗口移动的频率
      - 当移动间隔的时间跟窗口的大小刚好相等时，这种窗口被认为滚动窗口. 两个窗口之间是刚好衔接，没有任何的重叠或者间隙
      - 当有事件发生就移动一次，这种认为是滑动窗口。两个窗口之间会有重叠
   - 窗口可更新的时长，该问题主要对于延迟比较严重的事件到来时应该如何处理. 比如10:00-11:00之间的事件已经处理过了，但是在15:30时收到了一个发生在 10:00-11:00之间的事件，那么需不需要处理呢？对于这种情况可以定义个时间范围，比如只接受1小时的时间偏差的时间，当超过该范围的事件直接忽略


<br />

<a name="gWkgN"></a>
#### 流式处理的设计模式
 
流式处理的设计模式大概有以下几种

- 单事件处理模式
- 使用本地状态
- 多阶段和重分区
   - 该类模型，一般用于需要多次处理得出一个最终结果的业务，并且每一个下游阶段相比上游阶段来说，其数据量都会少很多
   - 比如对于统计每天涨跌幅度最大的前十的股票. 
      - 每个实例，不可能处理所有的股票，因为数据太多，实力不允许. 所以股票会根据其代号被分配到不同的实例去处理
      - 处理好之后，将该股票的涨跌幅度发送到一个只包含单分区的新主题上
      - 这样新主题的分区收到的数据所包含的内容将很少，只有股票的基本信息，所以能扛住
   - 其实这就是一个map-reduce的过程
- 使用外部查找-流表的连接
   - 这里一般是需要对流信息做一定的数据补充，该数据又存储在数据库中. 比如希望对用户的点击事件做数据丰富，将用户的一些基本信息，比如性别/年龄等信息补充上来. 
   - 此时不可能在流处理中去查询数据库，原因有二
      - 这样会对流式处理造成较大的时延
      - 同时由于事件流的数据量是不可预估的，如果查询将会对数据库造成非常大的压力，从而会影响其他的业务
   - 所以一般的处理是，将数据缓存到本地，然后在数据库有变化时，更新缓存. 即捕捉数据库的变更并形成事件流，该过程被称为CDC (Change Data Capture)
- 流与流的连接
   - 当需要将两个流的同一个key在同一个时间点发生的事件连接起来时，就需要使用流与流的连接. 比如希望看到用户搜索了什么并且从搜索结果中点击了什么，以更精准地建立用户画像，给用户推送相关信息. 
   - 处理的过程大致如下: 相同的key会被分配到同一个任务中，然后该任务就可以获取到该key的所有的事件，并做相关的处理
- 重新处理
   - 当流式处理应用出现了bug，此时对其进行升级改进。那么对于之前处理过的数据该如何更正呢？在kafka中，每个事件都会被持久化起来. 这样当改造之后的流式应用新版本上线之后，可以
      - 将新版本的应用当作一个新的消费者群组
      - 让它从输入topic的起始偏移量开始处理数据
      - 检查结果流，当新版本的应用程序赶上进度时，将客户端应用程序从旧版本的流式应用切换到新版本


<br />
<br />


